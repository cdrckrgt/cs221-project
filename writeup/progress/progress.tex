\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
% \usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
\usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amsmath}
\usepackage{graphicx}       % including images
\graphicspath{ {./images/} }
\usepackage{algorithm}
\usepackage{algpseudocode}  % including algorithms

\title{Deep Reinforcement Learning and Transfer Learning with FlappyBird}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Cedrick Argueta, Austin Chow, Cristian Lomeli \\
  Department of Computer Science\\
  Stanford University\\
  Stanford, CA 94305 \\
  \texttt{\{cedrick, archow, clomeli\}@cs.stanford.edu} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}

Reinforcement learning's growth in popularity in recent years is partly due to its ability to play some video games with a level of mastery that no human can reach. 
Transfer learning is popular in the field of deep learning, and using pre-trained models on certain tasks speeds up training time and increases performance significantly. 
In this project we aim to apply transfer learning to the popular video game \textit{FlappyBird} and analyze its performance compared to traditional reinforcement learning algorithms.
 
\end{abstract}


\section{Introduction}
Reinforcement learning is a technique for solving certain decision tasks, where an agent attempts to maximize a long-term reward through trial and error. 
This trial and error paradigm is better explained through the lens of exploration and exploitation: the agent must decide whether it explores and learns new information, or exploits the current policy to maximize reward.
We will primarily explore the applicability of deep Q-learning, a deep reinforcement learning algorithm, to the FlappyBird game.
In addition, we intend to explore the impact of transfer learning, a machine learning technique where a model developed for a specific task is reused as the starting point for a model in a second task. 
Being able to transfer learn will allow us to develop a more general model for different tasks, saving time and effort.
In particular, we aim to see whether transfer learning will speed up training time to reach the same performance level as the DQN model.

\section{Related Work}
The impetus behind our project is DeepMind's paper, "Playing Atari with Deep Reinforcement Learning". Mnih et al. were able to show how a Deep Q-Network could take raw image pixels from an Atari game and estimate the $Q$ function. \cite{deepmind}
Their model also takes advantage of recent advances in convolutional neural networks for image processing, a target network for stabilizing policy updates, and experience replay for a more efficient use of previous experience.
Transfer learning is the idea that generalization occurs both within tasks and across tasks. \cite{transfer}
Deep reinforcement learning may benefit from transfer learning, especially since convolutional neural networks are often used for playing games.
Since convolutional neural networks often learn similar features in the first, second, etc. layers across a range of image classification tasks, it's possible that transfer learning in the context of reinforcement learning for video games can exploit this.

\section{Game Mechanics}

The game of FlappyBird can be described as follows: a bird flies at a constant horizontal velocity $v_x$ and a variable veritcal velocity $v_y$. 
The bird can flap its wings, giving it a boost in altitude and velocity $v_y$.
The aim of the game is to avoid randomly generated pipes that restrict the flying area, leaving only a small gap for the bird to fly through. 

We model the problem as a Markov decision process with no knowledge of the transition probabilities or reward function at every state.
The transition probabilities are unknown, since each state consists of the deterministic bird position and velocity, along with the non-deterministic pipe positions.
The only reward signal received from the game in its standard implementation is when the bird flies past a pipe, giving us a reward of $1$. 
This sparse reward makes it impossible for us to get an explicit reward for each $(s, a, s')$ tuple.
The start state $s_{start}$ is the bird at some constant height, with no pipes on the screen.
The actions for every state are the same: the bird can flap its wings or not.
The only exception to this are the end states $s_{end}$, where the bird collides with a pipe or the ground.
In these cases, there are no actions to take, and the episode ends.

We have a similar model for each game that we experiment with: \textit{Pong, Catcher}, and \textit{PixelCopter}.

\section{Approach}

The goal of our project is twofold: we aim to evaluate deep reinforcement learning algorithms on the FlappyBird game, and also experiment with transfer learning to analyze the impact it makes on the training process. 

Vanilla learning methods like Q-learning are not well suited to this problem, since the state space of the game is very large. The position of the bird and pipes are continuous values, and as such we have an almost-zero probability of reaching a particular state that we've seen before.
Thus, it will be necessary to use either function approximation to generalize to unseen states or some form of deep learning that can extract features automatically.
We envision using Deep Q-networks as a nice foray into deep learning, given our experience with Q-learning.
The DQN model uses a neural network to approximate the Q action-value function, and allows for better generalization than the standard Q-learning approach.

Furthermore, we will demonstrate the ability for policies learned through deep reinforcement learning on  other games to transfer to FlappyBird.
This has been demonstrated before, particularly through the use of convolutional neural networks for playing games directly from pixels. \cite{deepmind}
In our case specifically, we will develop a Q-network parameterized by weights $\theta_{FlappyBird}$ that performs 'well enough' on FlappyBird, along with Q-networks parameterized by $\theta_{Pong}, \theta_{Catcher}, \theta_{PixelCopter}$ that perform 'well enough' on the games Pong, Catcher, and PixelCopter.
These networks will be initialized through Xavier initialization. \cite{xavier}
Once we have this, we will begin training new Q-networks parameterized by $\theta'_{Pong}, \theta'_{Catcher}, \theta'_{PixelCopter}$ that were initialized with $\theta_{FlappyBird}$.
We do this by fixing weights up until the final hidden layer, and solely training those weights.
This gives the effect of reducing the number of parameters needed for optimization, making our training faster and more efficient.
We will then compare the training times and absolute performance of the $\theta$ and $\theta'$ models, demonstrating the increase in efficiency or performance that transfer learning entails.

\subsection{Infrastructure}

The infrastructure for the game comes mostly from the PyGame Learning Environment, OpenAI gym, and keras-rl packages. 
The PyGame Learning Environment provides a nicely wrapped implementation of FlappyBird, complete with sprites and the relevant game mechanics built in. \cite{ple} \cite{openaigym} \cite{ale}
Keras-rl provides a deep reinforcement learning framework that gives us a simple interface for training agents. 
We take advantage of these two here, writing simple wrappers for the PLE game instance so that it is compatible with keras-rl. \cite{kerasrl}
The keras-rl package additionally provides an implementation of several algorithms that are applicable to FlappyBird, particularly Deep Q-networks and the SARSA algorithm.
For the transfer learning portion of the project, we saved weights info \texttt{hdf5} files with the Keras package.
The neural networks and much of the other deep learning architecture was also created in Keras, with a TensorFlow backend. \cite{keras} \cite{tensorflow}
This allowed us to monitor training time and other metrics inside TensorBoard.

Code for our project can be found at this link: \href{https://github.com/cdrckrgt/cs221-project} {https://github.com/cdrckrgt/cs221-project}.

\subsection{Baseline and Oracle}
The baseline for this project is an agent trained with SARSA on an $\epsilon$-greedy policy $\pi_{\epsilon}$.
The policy learned from SARSA algorithm described in Sutton and Barto updates the policy greedily based on $Q_{\pi_{\epsilon}}$. \cite{rl}
We see that $\pi_{\epsilon}$ will rarely get past the first pipe, even after 500 training episodes.
The policy performs similarly to a random policy, albeit it does maximize the reward that can be attained without passing the first pipe by jumping and dying after hitting the first pipe.
The rewards structure for the baseline is as follows:
\begin{center}
\begin{tabular} { | c | c | }
    \hline
    tick & $0.1$ \\
    \hline
    passed pipe & $1.0$ \\
    \hline
    collided & $-10.0$ \\
    \hline
\end{tabular}
\end{center}
where the agent receives a passive reward of $0.1$ for being alive each tick, and additional rewards for passing pipes and colliding with objects.

Another baseline for this project is an inexperienced human player. 
We have consistently been able to reach 10 pipes, but still occasionally fail earlier.

The oracle for this project is the high score of a human who can play the game extremely well.
Since the game isn't particularly difficult for humans and gameplay doesn't change dramatically throughout the game, it's possible for humans to play nearly indefinitely.

The gap between the baseline and oracle shows us that there is room for improvement, and deep Q-learning will be able to bridge that gap.

\section{Model}

In this section, we describe the DQN algorithm and how it applies to our task. We use a modified version of the algorithm described in the 2013 DeepMind paper.

Again, we model the environment $\mathcal{E}$ as a Markov decision process with no knowledge of the transition probabilities or reward function at every state.
There is a set of actions that may be taken at every time-step, designated $\mathcal{A}$.
Each interaction between the agent and its environment at time $t$ produces an observation $x_t \in \mathbb{R}^{d}$ and reward $r_t$, where $d$ is the dimensionality of the environment.

As in the standard Q-learning algorithm, we consider sequences of observations, actions, and rewards $x_1, a_1, r_1, \dots, x_t, a_t, r_t$ as the state of $\mathcal{E}$ at time $t$. 
The objective of the agent is to maximize its future reward, defined as:
\begin{equation}
\displaystyle R_t = \sum_{t' = t}^{T} \gamma^{t' - t}r_{t'}
\end{equation}
such that $T$ is the termination time-step of the game.
Then the optimal Q-value function is defined as follows:
\begin{equation}
\displaystyle Q^{*}(s, a) = \max_{\pi}\mathbb{E}[R_t\ |\ s_t = s, a_t = a, \pi]
\end{equation}
such that $\pi$ is a policy that maps states to actions.
So far we have described steps to arriving at the Q-learning algorithm described by Dayans and Watkins. \cite{qlearning} 
The DQN algorithm presented by DeepMind makes numerous improvement on this algorithm by introducing a non-linear function approximator and experience replay.

\subsection{The Q-network}
Since the vanilla Q-learning algorithm does not generalize well to state spaces as large as those encountered in Atari games, we use a neural network to approximate the Q-value function. 
Then we have a neural network parametrized by weights $\theta$ such that $Q(s, a; \theta) \approx Q^{*}(s, a)$.
At every iteration of training $i$, we minimize the squared error between the current Q-function and the optimal Q-function, according to the loss function:
\begin{equation}
\displaystyle L_i(\theta_i) = \mathbb{E}_{s, a \sim \rho(\cdot)}[(y_i - Q(s, a; \theta_i))^2]
\end{equation}
where $y_i = \mathbb{E}_{s' \sim \mathcal{E}}[r + \gamma\max_{a'}Q(s', a'; \theta_{i - 1}\ |\ s, a]$ and $\rho(s, a)$ is a probability distribution over states and actions. 
The intuition behind this loss function is that we are minimizing the distance between our target, which is the maximum expected future reward for this state and the previous iteration's weights, and our current Q value.
We are effectively getting closer and closer to the true Q values every training iteration.
To minimize this objective function, we update the weights according to the gradient:
\begin{equation}
\displaystyle \nabla_{\theta_i}L_i(\theta_i) = \mathbb{E}_{s, a \sim \rho(\cdot)}[(r + \gamma \max_{a'}Q(s', a'; \theta_{i - 1}) - Q(s, a; \theta_i))\nabla_{\theta_i}Q(s, a; \theta_i)]
\end{equation}
And we've now arrived at the Deep Q-network algorithm that we can apply to our task.
It's important to note that the original DQN paper applies this algorithm to raw pixel vectors as each $x_t$, whereas we use the internal game state for each $x_t$.
This allows us to achieve the same level of performance in a much quicker fashion, since we don't need to spend the extra compute power on training a convolutional neural network to learn features for each game.
The algorithm, lifted from Mnih's paper, is described in \ref{alg:dqn}.

\begin{algorithm}
\caption{Deep Q-learning with Experience Replay}
\label{alg:dqn}
\begin{algorithmic}[1]
\State Initialize replay memory $\mathcal{D}$ to capacity $N$
\State Initialize action-value function $Q$ with random weights
\For{$\textrm{episode} = 1, M$}
    \State Initialize sequence $s_1 = \{x_1\}$ and preprocessed sequenced $\phi_1 = \phi(s_1)$
    \For{$t = 1, T$}
        \State With probability $\epsilon$ select a random action $a_t$
        \State otherwise select $a_t = \max_{a}Q^{*}(\phi(s_t), a; \theta)$
        \State Execute action $a_t$ in emulator and observe reward $r_t$ and observation $x_{t + 1}$
        \State Set $s_{t + 1} = s_t, a_t, x_{t + 1}$ and preprocess $\phi_{t + 1} = \phi(s_{t + 1})$
        \State Store transition $(\phi_t, a_t, r_t, \phi_{t + 1})$ in $\mathcal{D}$
        \State Sample random minibatch of transitions $(\phi_j, a_j, r_j, \phi_{j + 1})$
        \State Set $ \displaystyle  y_j = 
                        \begin{cases}
                            r_j & \textrm{for terminal} \phi_{j + 1} \\
                            r_j + \gamma\max_{a'}Q(\phi_{j + 1}, a'; \theta) & \textrm{for non-terminal} \phi_{j + 1} \\
                        \end{cases}
                   $
        \State Perform a gradient descent step on $(y_j - !(\phi_j, a_j; \theta))^2$ according to 
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Experience Replay}
Additionally, the algorithm presented in Mnih's paper makes use of experience replay to smooth learning and prevent divergence.
Rather than performing gradient updates on each consecutive sample, samples are stored in a ring buffer and sampled randomly during the learning phase of the algorithm. 
This has the effect of improving sample efficiency and removing the dependence of each training update on the previous training update.

\subsection{Target Network}
Finally, we make use of a target network to further stabilize the training process. 
Note that during our gradient descent, the weights that produce our target values shift every iteration.
Instead of using these rapidly shifting weights, we copy the parameters of the training network $\theta_{training}$ to a second, identical model $\theta_{target}$.
With this identical model, we may compute the same targets, albeit with less variance by only updating the $\theta_{target}$ to match the $\theta_{training}$ with some fixed, low probability at every iteration.

\subsection{Model Architecture}
Since we use as input to the Q-network the internal game state representation as opposed to the raw pixel inputs, we don't make use of a convolutional neural network to learn features directly from the game's pizels.
We instead make use of a multi-layered perceptron that takes as input the game's state in vector form.
In the case of FlappyBird, the state vector is a vector in $\mathbb{R}^8$, and contains the bird's position, velocity, and the distance, top, and bottom positions of the next two pipes.
The dimensionality of the input differed for each game we worked with, but we kept the dimensionality of the hidden layers the same for each game so that we could experiment with transfer learning.
The outputs of the Q-network for each game were the Q-values of all actions in the action space for that game.
In the case of FlappyBird, the output was two Q-values in $\mathbb{R}$, one for the action to jump, and the null action.

\section{Preliminary Results}

\subsection{Vanilla DQN}
After finalizing the DQN algorithm, we experimented with different hyperparameters and had to define performance metrics to compare the base DQN models with the transfer learned models. 
The hyperparameters for the models trained from scratch are shown in table \ref{hyperparams-xavier}.
We see in the graph \ref{fig:flappy-training} the variance in episode reward over different random seeds.
The DQN performs well above the ability of the average human, since it averages scores in the thousands while testing.
Reaching this level of performance required, on average, $20000$ iterations.

\begin{table}[h!]
\centering
\begin{tabular}{ll}

\multicolumn{2}{ c }{ \textbf{Hyperparameters for Xavier-initialized networks} } \\

\\

    \begin{tabular}{| c | c |}
    \hline
    \multicolumn{2}{| c |}{FlappyBird Hyperparameters} \\ 
    \hline
    Target Model Update & 1000 \\
    \hline
    Learning Rate & 1000 \\
    \hline
    $\dots$ & $\dots$ \\
    \hline
    tick & $0.1$ \\
    \hline
    passed pipe & $1.0$ \\
    \hline
    collided & $-10.0$ \\
    \hline
    \end{tabular}

&

    \begin{tabular}{| c | c |}
    \hline
    \multicolumn{2}{| c |}{Pong Hyperparameters} \\ 
    \hline
    Target Model Update & 1000 \\
    \hline
    Learning Rate & 1000 \\
    \hline
    $\dots$ & $\dots$ \\
    \hline
    tick & $0.1$ \\
    \hline
    passed pipe & $1.0$ \\
    \hline
    collided & $-10.0$ \\
    \hline
    \end{tabular}

\\
\\

    \begin{tabular}{| c | c |}
    \hline
    \multicolumn{2}{| c |}{Catcher Hyperparameters} \\ 
    \hline
    Target Model Update & 1000 \\
    \hline
    Learning Rate & 1000 \\
    \hline
    $\dots$ & $\dots$ \\
    \hline
    tick & $0.1$ \\
    \hline
    passed pipe & $1.0$ \\
    \hline
    collided & $-10.0$ \\
    \hline
    \end{tabular}

&

    \begin{tabular}{| c | c |}
    \hline
    \multicolumn{2}{| c |}{PixelCopter Hyperparameters} \\ 
    \hline
    Target Model Update & 1000 \\
    \hline
    Learning Rate & 1000 \\
    \hline
    $\dots$ & $\dots$ \\
    \hline
    tick & $0.1$ \\
    \hline
    passed pipe & $1.0$ \\
    \hline
    collided & $-10.0$ \\
    \hline
    \end{tabular}

\\

\end{tabular}
\caption{These hyperparameters were obtained with lots of trial and error.}\label{hyperparams-xavier}
\end{table}

\begin{figure}[h!]
\includegraphics[width=0.5\textwidth]{test-tensorboard}
\label{fig:flappy-training}
\end{figure}

\begin{figure}[h!]
\includegraphics[width=0.5\textwidth]{test-tensorboard}
\label{fig:other-training}
\end{figure}

To experiment with the effectiveness of transfer learning, we separately trained Q-networks on the games Pong, Catcher, and PixelCopter.
Additionally, we show the training times for the Xavier-initialized models in figure \ref{fig:other-training}.

\subsection{Transfer Learning}
We show here the effectiveness of using pre-trained weights from FlappyBird on the games Pong, Catcher, and PixelCopter.

\bibliography{ref}
\bibliographystyle{plain}


\end{document}

